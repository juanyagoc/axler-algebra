% !TEX TS-program = latexmk

\documentclass{scrartcl}
\usepackage[fancyproofs,fancytheorems,noindent]{juan}
\usetikzlibrary{graphs, arrows.meta, positioning}

\title{Linear Algebra}
\author{Juan Yago Castells}
\date{\today}

\begin{document}

\textbf{\Large Linear Algebra}
\vspace*{0.5cm}

Vector Spaces, Linear Transformations and Innver Product Spaces.

Definitions, theorems and exercises from the fourth edition of the book
\textit{Linear Algebra Done Right} by Sheldon Axler.

\section{Subespacios invariantes}%

\begin{definition}[Subespacio invariante]
Suponiendo $T \in \mathcal{L}(V)$. Un subespacio $U$ de $V$ es llamado \vocab{invariante} bajo $T$ si $u \in U$ implica $Tu \in U$.
\end{definition}
En la búsqueda del subespacio no trivial más simple posible (1-dimensional) nos encontramos con un $U$ definido como
\begin{align*}
  U = \{ \lambda v : \lambda \in \F \} = \vecspan(v)
\end{align*}
Vemos que si $U$ es invariante bajo un operador $T\in \mathcal{L}(V)$ entonces $Tv \in U$ y por tanto hay un escalar $\lambda \in \F$ que cumple
\begin{align*}
  Tv = \lambda v
\end{align*}
Esta ecuación es tan importante que el vector $v$ y el valor $\lambda$ reciben su propio nombre.

\section{Vectores y valores propios}

\begin{definition}[Valor Propio o Eigenvalue]
  Suponiendo $T \in \mathcal{L}(V)$. Un número $\lambda \in \F$ es llamado \vocab{valor propio} de $T$ si existe $v \in V$ tal que $v \neq 0$ y $Tv = \lambda v$.
\end{definition}

Es condición indispensable que $v \neq 0$ porque cualquier escalar $\lambda \in \F$ cumple $T0 = \lambda 0$.

\begin{definition}[Vector Propio o Eigenvector]
  Suponiendo $T \in \mathcal{L}(V)$ y $\lambda \in \F$ es un valor propio de $T$. Un vector $v \in V$ es llamado \vocab{vector propio} de $T$ correspondiente a $\lambda$ si $v \neq 0$ y $Tv = \lambda v$.
\end{definition}

\begin{theorem}[Una lista de vectores propios es linealmente independiente]
  \label{teo:indep}
  Sea $T \in \mathcal{L}(V)$. Supón $\lambda_1, \ldots ,\lambda_m$ son distintos valores propios de $T$ y $v_1, \ldots , v_m$ son los correspondientes vectores propios. Entonces $v_1, \ldots ,v_m$ es linealmente independiente. 
\end{theorem}

\begin{proof}
Suponeos que $v_1,\ldots ,v_m$ es linealmente dependiente. Siendo $k$ el entero positivo más pequeño tal que
  \begin{align}
  v_k \in span(v_1,\ldots ,v_{k-1}); \tag{5.11} \label{eq:5.11}
  \end{align}
  la existencia de $k$ con esta propiedad se sigue del \emph{Lema de Dependencia Lineal} (2.21). Por tanto existe $a_1,\ldots ,a_{k-1} \in \F$ tal que
  \begin{align}
    v_k = a_1 v_1 + \cdots + a_{k-1}v_{k-1}. \tag{5.12} \label{eq:5.12}
  \end{align}
  Applicando $T$ a ambos lados de la ecuación obtenemos
\begin{align*}
  \lambda_k v_k = a_1 \lambda_1 v_1 + \cdots + a_{k-1}\lambda_{k-1}v_{k-1}.
\end{align*}
Multiplicando ambos lados de \ref{eq:5.12} por $\lambda_k$ y luego restando la ecuación de arriba obtenemos
\begin{align*}
  0 = a_1 (\lambda_k - \lambda_1)v_1 + \cdots + a_{k-1}(\lambda_k - \lambda_{k-1})v_{k-1}.
\end{align*}
Dado que definimos $k$ como el menor entero positivo que satisface \ref{eq:5.11}, $v_1,\ldots ,v_{k-1}$ es linealmente independiente. Por tanto la ecuación de arriba implica que todas las $a$'s son 0. Sin embargo, esto significa que $v_k$ es igual a 0, contradiciendo nuestra hipotesis de que $v_k$ es un vector propio. Por tanto nuestra asunción de que $v_1, \ldots ,v_m$ es linealmente dependiente es falsa.
\end{proof}

\begin{theorem}[máximo de valores propios]
Suponiendo $V$ finito-dimensional. Cada operador en $V$ tiene como mucho dim$V$ valores propios distintos.
\end{theorem}

%\begin{proof}
%  Sea $T \in \mathcal{L}(V)$. Suponiendo que $\lambda_1, \ldots ,\lambda_m$ son distintos valores propios de $T$. Sea $v_1,\ldots ,v_m$ los correspondientes vectores propios. El teorema \ref{teo:indep} implica que la lista $v_1,\ldots ,v_m$ es linealmente independiente. Por tanto $m \leq$ dim$V$, como se deseaba.
%\end{proof}
%
%\begin{proposition}[la invertibilidad se mantiene en la multiplicación]
%  Suponiendo que $V$ es finito-dimensional y $S,\ T \in \mathcal{L}(V)$. Probar que $ST$ es invertible si y solo si ambos $S$ y $T$ son invertibles.
%\end{proposition}
%\begin{proof}
%  ($\Leftarrow$) Suponiendo que $S$ y $T$ son invertibles, por el \emph{Problema 1}, $ST$ es también invertible.\\
%  ($\Rightarrow$) Suponiendo que $ST$ es invertible. Demostraremos que $T$ es inyectiva y $S$ sobreyectiva. Como $V$ es finito-dimensional, esto implicaría que tanto $S$ como $T$ son invertibles. Entonces suponiendo $v_1,v_2 \in V$ son tales que $Tv_1 = Tv_2$. Luego $(ST)(v_1) = (ST)(v_2)$, y como $ST$ es invertible (y por tanto inyectiva), debemos tener que $v_1 = v_2$, por tanto $T$ es inyectiva. Siguiente, suponiendo $v \in V$. Como $T^{-1}$ es sobreyectiva, existe $w \in V$ tal que $T^{-1}w = v$. Y como $ST$ es sobreyectiva, existe $p \in V$ tal que $(ST)(p) = w$. A esto le sigue que $(STT^{-1})(p) = T^{-1}(w)$, y por tanto $Sp = v$. Luego $S$ es sobreyectiva, completando la prueba.
%\end{proof}

\subsection{Definiciones clave para el calculo de valores propios}

\begin{definition}
  Las siguientes afirmaciones para un operador $T\in \mathcal{L}(V)$, con $V$ de dimensión finita, y un escalar $\lambda \in \F$ son equivalentes:
\begin{enumerate}[label=(\alph*)]
    \item $\lambda$ es un valor propio de $T$;
    \item $T - \lambda I$ no es inyectivo;
    \item $T - \lambda I$ no es sobreyectivo;
    \item $T - \lambda I$ no es invertible.
\end{enumerate}
\end{definition}

\begin{theorem}[Teorema multiplos de 3]
Para todo $n \in \Z$ se cumple que al menos uno de los factores de la expresión $n(n+1)(n+2)$ es divisible por 3. 
\end{theorem}
\begin{proof}
Vamos a completar la prueba por inducción, es fácil ver que el teorema se cumple para el caso $n=1$, $1 \cdot 2 \cdot 3 = 3 \cdot (2)$. Ahora suponiendo que se cumple para $n$ demostraremos que lo hace también para $n+1$. Con la expresión
\begin{align*}
(n+1)(n+2)(n+3) = 3k
\end{align*}
Para cierto $k \in \Z$, desarrollando la expresión obtenemos
\begin{align*}
(n+1)(n+2)(n+3) &= \frac{3k}{n}(n+3) \\
			    &= 3 \cdot \frac{k}{n}(n+3)
\end{align*}
Donde la primera igualdad se sostiene de la supocisión inductiva. Vemos que si $\frac{k(n+3)}{n}$ es un entero entonces hemos terminado la prueba y sabemos que es un entero ya que de la suposición inductiva sabemos que
\begin{align*}
\frac{k(n+3)}{n} &= \frac{kn+3k}{n} \\
				 &= \frac{kn + n(n+1)(n+2)}{n} \\ 
				 &= k + (n+1)(n+2)
\end{align*}
Por tanto $\frac{k(n+3)}{n}$ es un entero completando la prueba
\end{proof}

\section{Singular Value Decomposition (SVD)}

\begin{definition}[SVD]
  Suppose $T \in \mathcal{L}(V,W)$ and the positive singular values of $T$ are $s_1, \ldots , s_m$. Then there exist orthonormal lists $e_1, \ldots , e_m$ in $V$ and $f_1, \ldots ,f_m$ in $W$ such that
  \[
    Tv = s_1\langle v, e_1 \rangle f_1 + \cdots + s_m\langle v,e_m \rangle f_m \tag{3.11} \label{eq:3.11}
  \]
for every $v \in V$.
\end{definition}

\begin{proof}
  Let $s_1, \ldots ,s_m$ denote the singular values of $T$ (thus $n = dimV$). Because $T^* T$ is a positive operator, the spectral theorem implies that there exists an orthonormal basis $e_1, \ldots ,e_n$ of $V$ with
  \[
    T^* Te_k = s^2_k e_k \tag{3.12} \label{eq:3.12}
  \]
for each $k = 1, \ldots ,n$.

\quad For each $k = 1,\ldots ,m$, let
\[
  f_k = \frac{Te_k}{s_k}. \tag{3.13} \label{eq:3.13}
\]
If $j,k \in {1,\ldots ,m}$, then
\[
  \langle f_j,f_k \rangle = \frac{1}{s_j s_k} \langle Te_j, Te_k \rangle = \frac{1}{s_j s_k}\langle e_j, T^* T e_k \rangle = \frac{s_k}{s_j}\langle e_j, e_k \rangle = \begin{cases} 0 \hspace{10px} if\ j \neq k,\\ 1 \hspace{10px} if\ j = k. \end{cases}
\]
Thus $f_1,\ldots ,f_m$ is an orthonormal list in $W$.

\quad If $k \in {1,\ldots ,n}$ and $k > m$, then $s_k = 0$ and hence $T^* Te_k = 0$ (by \ref{eq:3.12}), which implies that $Te_k = 0$.

\quad Suppose $v\in V$. Then
\begin{align*}
  Tv &= T \left( \langle v,e_1 \rangle e_1 + \cdots + \langle v,e_n \rangle e_n \right) \\
     &= \langle v,e_1 \rangle Te_1 + \cdots + \langle v,e_m \rangle Te_m \\
     &= s_1 \langle v,e_1 \rangle f_1 + \cdots + s_m \langle v,e_m \rangle f_m,
\end{align*}

where the last index in the first line switched from $n$ ot $m$ in the secod line because $Te_k = 0$ if $k > m$ (as noted in the paragraph above) and the third line follows from \ref{eq:3.13}. The equation above is our desired result.
\end{proof}

With the tool presented above we can arrive to a very useful concept in copression theory and computation, wich is the appoximation by linear maps with lower-dimensional range.

\begin{definition}[best approximation by linear map whose range has dimension $\le k$]
  Suppose $T \in \mathcal{L}(V,W)$ and $s_1 \geq \cdots \geq s_m$ are the positive singular values of $T$.

  Suppose $1 \le k < m$. Then
  \[
    min\{ \| T-S\| :S \in \mathcal{L}(V,W)\ \text{and dim range } S \le k\} = s_{k+1}.
  \]
  Furthermore, if
  \[
    Tv = s_1\langle v,e_1\rangle f_1 + \cdots + s_m\langle v,e_m\rangle f_m
  \]
  is a singular value decomposition of $T$ and $T_k \in \mathcal{L}(V,W)$ is defined by
  \[
    T_k v = s_1\langle v,e_1\rangle f_1 + \cdots + s_k\langle v,e_k\rangle f_k
  \]
  for each $v\in V$, then dim range $T_k = k$ and $\| T-T_k\| = s_{k+1}$.
\end{definition}
\begin{proof}
  If $v \in V$ then
\begin{align*}
{\|(T-T_k)v\|}^2 &= {\|s_{k+1}\langle v,e_{k+1}\rangle f_{k+1}+\cdots +s_m\langle v,e_m\rangle f_m \|}^2 \\
               &= s_{k+1}^2{|\langle v,e_{k+1}\rangle |}^2 +\cdots +s_m^2{|\langle v,e_m\rangle |}^2 \\
               &\le s_{k+1}^2\left( {|\langle v,e_{k+1}\rangle |}^2+\cdots + {|\langle v,e_m\rangle |}^2\right) \\
               &\le s_{k+1}^2{\|v\|}^2.
\end{align*}
Thus $\|T-T_k\| \le s_{k+1}$. The equation $(T-T_k)e_{k+1} = s_{k+1}f_{k+1}$ now shows that $\|T-T_k\| \le s_{k+1}$.

  Suppose $S \in \mathcal{L}(V,W)$ and dim range $S \le k$. Thus $Se_1,\ldots ,Se_{k+1}$, which is a list of length $k + 1$, is linearly dependent. Hence there exist $a_1,\ldots ,a_{k+1} \in \mathbb{F}$, not all 0, shut that
\[
  a_1Se_1 + \cdots + a_{k+1}Se_{k+1} = 0.
\]
Now $a_1Se_1 + \cdots + a_{k+1}Se_{k+1} \ne 0$ because $a_1,\ldots ,a_{k+1}$ are not 0. We have
\begin{align*}
\|(T - S)(a_1 e_1 + \cdots + a_{k+1} e_{k+1})\|^2 
  &= \|T(a_1 e_1 + \cdots + a_{k+1} e_{k+1})\|^2 \\
  &= \|s_1 a_1 f_1 + \cdots + s_{k+1} a_{k+1} f_{k+1}\|^2 \\
  &= s_1^2 |a_1|^2 + \cdots + s_{k+1}^2 |a_{k+1}|^2 \\
  &\ge s_{k+1}^2 (|a_1|^2 + \cdots + |a_{k+1}|^2) \\
  &= s_{k+1}^2 \|a_1 e_1 + \cdots + a_{k+1} e_{k+1}\|^2.
\end{align*}
Because $a_1e_1 + \cdots + a_{k+1}e_{k+1} \ne 0$, the inequality above implies that
\[
  \|T-S\| \ge s_{k+1}.
\]
Thus $S = T_k$ minimizes $\|T-S\|$ among $S \in \mathcal{L}(V,W)$ with dim range $S \le k$.
\end{proof}
\begin{problem}
  Fix $u,x \in V$ with $u \ne 0$. Define $T \in \mathcal{L}(V)$ by $Tv = \langle v,u\rangle x$ for every $v \in V$.

  Prove that
  \[
  \sqrt{T^*T}v = \frac{\|x\|}{\|u\|}\langle v,u\rangle u
  \]
  for every $v \in V$.
\end{problem}

\newpage

\section{QR Decomposition using Householder reflections}

\begin{definition}[Householder operator]
The Householder operator $H$ in a inner product space $V$ represents a reflectionof a given vector $x$ over a plane perpendicular to a unitary vector $u$, wich means $\| u\| = 1$. so H applied to x has de form:
\[
  H(x) = x - 2\text{proj}_u(x)
\]
  Knowing the norm of $u$ is 1 and that $u^*$ represents the conjugate transpose which in the real field $\mathbb{R}$ equals the transpose $u^T$ we end up with
\[
  H(x) = x - 2 u^* x u = (I - 2 u^* u)x
\]
With this we see that the Householder matrix $H$ representing a reflection over a normal vectoro $u$ has the form
\[
  H = I - 2uu^*
\]
\end{definition}

\begin{definition}[QR Decomposition]
  Given a matrix $A$ with $m$ rows and $n$ columns. Then there exists a unique way of decomposing A in a combination of a unitary matrix named $Q$ and an upper triangular matrix $R$, so A can be expressed in the form
  \[
  A = QR
  \]
\end{definition}
\begin{proof}
  Suppose A is formed with the list $v_1,\ldots ,v_n$. By the Gram-Schmidt prodedure we can write any vector $v$ of the vector space $V$ in the form
  \[
  v = \langle v_1, e_1\rangle e_1 + \cdots + \langle v_n, e_n\rangle e_n
  \]
  where $e_1, \ldots ,e_n$ is an orthonormal list that spans $V$.

  Constructing $Q$ with the column vectors of this orthonormal basis say
  \[
  Q = \left( \begin{matrix}
                e_1 & e_2 & \cdots & e_n \\
             \end{matrix}
      \right)
  \]
  This $Q$ sqare matrix is unitary by construction with dimensions $n \times n$.

  Then, multiplying $Q$ by $R$ upper triangular with dimensions $n \times m$ formed by the inner products of each component of $v$ with its coresponding orthonormal vector the multiplication $QR$ gives us the Gram-Schmidt procedure.
\end{proof}

Now we can see that the unitary matrix $Q$ can be formed multiplying a series of Householder operators say $H_k$ with the objetive of transforming a matrix $A$ into a triangular matrix by a series of this Householder reflections in the form
\[
H_k \cdots H_1 A = R
\]
and therefore
\[
Q = (H_k \cdots H_1)^{-1} = H_1^* \cdots H_k^*
\]

The second equality holds because heach $H_j$ is itself unitary and the composition of unitary matrices gives a unitary matrix in this case $Q$.

Now in order to construct the desired $H_k$ matrix for reflecting an arbitrary vector $x$ on some other vector choosen in an specyfic way that the reflextion lands on a vector of the orthonormal base we have to follow some steps.

First choose the first normal column vector of the canonical base with the right dimension say $e_1 = (1, 0, \ldots, 0)^T$, next define a vector
\[
v = x + sign(x_1)\|x\|e_1,
\]
then we normalize this vector
\[
u = \frac{v}{\|v\|}
\]
and lastly the matrix is constructed like we defined it $H = I - 2uu^T$.

\begin{example}[Basic reflection of a given vector example]
  Given a vector say $x = (1, 2, 3)^T$ on $\mathbb{R}^3$ so the canonical basis vector we watn to choose is $e_1 = (1, 0, 0)^T$, we will obtain $v$ as explained:
\begin{align*}
    v &= x - \text{sign}(x_1)\|x\|e_1 \\
      &= x + \sqrt{1^2 + 2^2 + 3^2}e_1 \\
      &= (1, 2, 3)^T + \sqrt{14}e_1 \\
      &= (1 + \sqrt{14}, 2, 3)^T
\end{align*}
now to normalize $v$
\[
  u = \frac{v}{\|v \|} \\
    = \frac{(1 + \sqrt{14}, 2, 3)^T}{\sqrt{{(1 + \sqrt{14})}^2 + 13}}
\]
The reflection matrix $H$ will have the form
\[
  H = \begin{pmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1
    \end{pmatrix} - 2 \begin{pmatrix} u_1 \\ u_2 \\ u_3 \end{pmatrix}
      \left( u_1, u_2, u_3 \right)
\]
And with this we se that
\[
    Hx = \begin{pmatrix} -\sqrt{14} \\ 0 \\ 0 \end{pmatrix}
\]
which is a scalar multiple of a canonical vector as we desired.
\end{example}
This vector reflection can be applied to not just a single vector but to an entire matrix as it is formed with vectors allowing us to transform a given matrix into a lower triangular matrix.
\end{document}
